# -*- coding: utf-8 -*-
"""gemini_explorer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VmVoXmDqmjy_2FkdHVzszDPxbRPmjO3q
"""

!pip install streamlit

import vertexai
import streamlit as st
from vertexai.preview import generative_models
from vertexai.preview.generative_models import GenerativeModel, Part, Content, ChatSession

!pip install google-cloud-aiplatform

project = "gemini-explorer-412318"
vertexai.init(project=project)

config = generative_models.GenerationConfig(
    temperature=0.4
)
model = GenerativeModel(
    "gemini-explorer-412318",
    generation_config=config
)
chat = model.start_chat()

def llm_function(chat: ChatSession, query):
#Code for sending a query to the chat session, retrieving the response, and processing the output
    response = chat.send_message(query)
    output=response.candidates[0].content.parts[0].text
# Display the response in the Streamlit app
    st.chat_message("model")
    st.markdown(output)
# Update the session state with the messages if needed
    chat.update_session(model, output)

#load and display chat history
st.session_state.messages = st.session_state.get('messages',[])

import streamlit as st

st.title("Gemini Explorer")
if "messages" not in st.session_state:
    st.session_state.messages = []

import streamlit as st

if "messages" in st.session_state:
    for index, message in enumerate(st.session_state.messages):
        st.write(f"Message {index + 1}: {message}")
else:
    st.write("No chat history available.")

import streamlit as st

query = st.text_input("Gemini Flights are amazing")
if query:
  with st.chat_message("model"):
      st.markdown("output")
  llm_function(chat,query)